{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inputembedding(nn.Module):\n",
    "    def __init__(self,d_model:int , vocab_size : int):\n",
    "        super().__init__()\n",
    "        self.d_model=d_model\n",
    "        self.vocab_size=vocab_size\n",
    "        self.embedkding = nn.Embedding(vocab_size,d_model)\n",
    "    def forward(self,x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncodimg(nn.Module):\n",
    "    def __init__(self, d_model : int , seq_len : int , dropout : float ):\n",
    "        super().__init__()\n",
    "        self.d_model=d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "        \n",
    "        ## create a matrix of shape (seq , d_model )\n",
    "        pe= torch.zeros(seq_len, d_model)\n",
    "        # create vector of shape (seq_len )\n",
    "        position=torch.arange(0,seq_len,dtype=torch.float).unique(1)\n",
    "        div_term=torch.exp(torch.arange(0,d_model,2).float()*(-math.log(10000.0)/d_model))\n",
    "        # applay the sin to even position \n",
    "        pe[:,0::2]=torch.sin(position*div_term)\n",
    "        # applay thr cos   to the odd postion \n",
    "        pe[:,1::2]=torch.cos(position*div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0) ##(1 , deq_len , d_model)\n",
    "        self.register_buffer('pe',pe)\n",
    "    def forward(self , x):\n",
    "        x = x + (self.pe[:,:x.shape[1],:]).requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, eps:float=10**-6 )-> None :\n",
    "        super().__init__()\n",
    "        self.eps=eps\n",
    "        self.bias=nn.Parameter(torch.zeros(1))   # for additional \n",
    "        self.alpha=nn.Parameter(torch.ones(1)) # for multiplication\n",
    "    \n",
    "    def forward(self , x):\n",
    "        mean = x.mean(dim=-1,keepdim=True )\n",
    "        std = x.std(dim=-1,keepdim=True )\n",
    "        return self.alpha*(x-mean) / (std + self.eps) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FFN(x) = (x.W1 + b1 )*W2 + b2 W1(d_model,d_ff) and W2(d_ff , d_model )\n",
    "# we use it after the attention it s a fully connected feed forword network\n",
    "# applay for each position \n",
    "# d_model: The dimensionality of the input and output (e.g., 512 or 768 in many architectures).\n",
    "#d_ff: The dimensionality of the hidden layer. Typically much larger than d_model (e.g., 2048 in many architectures), allowing the model to learn richer feature transformations.\n",
    "#dropout: A regularization technique to prevent overfitting by randomly dropping some connections during training\n",
    "class  Feed_Forword(nn.Module):\n",
    "    def __init__(self, d_model : int , d_ff : int  , dropout :float):\n",
    "        super().__init__()\n",
    "        self.linear_1=nn.Linear(d_model,d_ff) # for W1 b1 \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2=nn.Linear(d_ff,d_model) # for W2 b2\n",
    "    def forward(self , x):\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttenstion(nn.Module):\n",
    "    def __init__(self, d_model : int ,h :int , dropout : float ):\n",
    "        super().__init__()\n",
    "        self.d_model=d_model\n",
    "        self.h=h\n",
    "        assert d_model%h==0,\"d_model is not devede by h   \"\n",
    "        self.d_k = d_model // h\n",
    "         # Learnable linear projections for queries, keys, values, and output\n",
    "        self.w_q = nn.Linear(d_model, d_model)  # For projecting queries\n",
    "        self.w_k = nn.Linear(d_model, d_model)  # For projecting keys\n",
    "        self.w_v = nn.Linear(d_model, d_model)  # For projecting values\n",
    "        self.w_0 = nn.Linear(d_model, d_model)  # Final output projection\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    @staticmethod\n",
    "    def calculat_attenstion( q , k , v , mask , dropout : nn.Dropout ):\n",
    "        attemstion_score= ((q@k.transpose(-2,-1)) / math.sqrt(q.shape[-1]))\n",
    "        if mask is not None:\n",
    "            attemstion_score.masked_fill_(mask==0,-100)\n",
    "        attemstion_score = attemstion_score.softmax(dim =-1)\n",
    "        if dropout is not None:\n",
    "            attemstion_score=dropout(attemstion_score )\n",
    "        return (attemstion_score @ v) , attemstion_score\n",
    "    def forward(self, q,k,v,mask ):\n",
    "        query = self.w_q(q)\n",
    "        key = self.w_k(k)\n",
    "        value = self.w_v(v)\n",
    "        # we are giong from (batch , seq , d_model ) to firs-> (batch , seq , h , d_k)\n",
    "        #  -> and then using transpose to switch between the secodes demension and \n",
    "        # the third so we get (batch , h , seq , d_k)\n",
    "        # the porpose to have (seq , d_k)\n",
    "        # so we did it for all the three\n",
    "        query=query.view(query.shape[0],query.shape[1], self.h , self.d_k).transpose(1,2)\n",
    "        key=key.view(key.key[0],key.shape[1], self.h , self.d_k).transpose(1,2)\n",
    "        value=value.view(value.value[0],value.shape[1], self.h , self.d_k).transpose(1,2)\n",
    "        x , attemstion_score = MultiheadAttenstion.calculat_attenstion(query,key,value,mask , self.dropout)\n",
    "        ## concat the heads now  \n",
    "        # first we trnaspose them to have the initale spreat (batch , seq , h , d_k)\n",
    "        x=x.transpose(1,2)\n",
    "        # now we should concat the h and d_k to have d_module\n",
    "        # Merge heads into d_model\n",
    "        x = x.contiguous().view(x.shape[0], x.shape[1], self.h*self.d_k) \n",
    "        # final target is \n",
    "        x= self.w_0(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualCongitnection(nn.Module):\n",
    "    def __init__(self, dropout : float):\n",
    "        super().__init__()\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.norm=LayerNormalization()\n",
    "    def forward(self, x,sublayer):\n",
    "        ## means that sublayer witch could be  represent multihead_attention_layer or\n",
    "        #  what ever gonna have the same Normalization as x \n",
    "        return x+self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, self_attentio : MultiheadAttenstion , feed_forword : Feed_Forword , dropout : float):\n",
    "        super().__init__()\n",
    "        self. self_attentio = self_attentio \n",
    "        self.feed_forword = feed_forword\n",
    "        self.connection = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n",
    "    def forward(self , x , src_masck):\n",
    "        x=self.connection[0](x, lambda x : self.self_attentio(x,x,x,src_masck))\n",
    "        x=self.connection[1](x, self.feed_forword)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layers : nn.ModuleList):\n",
    "        super().__init__()\n",
    "        self.layers=layers\n",
    "        self.norm = LayerNormalization()\n",
    "    def forward(self , x, mask):\n",
    "        for layer in self.layers:\n",
    "            x=layer(x , mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DencoderBlock(nn.Module):\n",
    "    def __init__(self, self_attentio : MultiheadAttenstion , feed_forword : Feed_Forword , dropout : float):\n",
    "        super().__init__()\n",
    "        self. self_attentio = self_attentio \n",
    "        self.feed_forword = feed_forword\n",
    "        self.connection = nn.ModuleList([ResidualConnection(dropout) for _ in range(3)])\n",
    "    def forward(self , x  , k  , v, src_masck , tgt_mask):\n",
    "        x=self.connection[0](x, lambda x : self.self_attentio(x,x,x,tgt_mask))\n",
    "        # result from the encoder  the k and the value \n",
    "        x=self.connection[1](x, lambda x : self.self_attentio(x,k,v,src_masck))\n",
    "        x=self.connection[2](x, self.feed_forword)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dencoder(nn.Module):\n",
    "    def __init__(self, layers : nn.ModuleList):\n",
    "        super().__init__()\n",
    "        self.layers=layers\n",
    "        self.norm = LayerNormalization()\n",
    "    def forward(self , x  , k  , v, src_masck , tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x=layer(x  , k  , v, src_masck , tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "    def __init__(self, d_model , vocab_size):\n",
    "        super().__init__()\n",
    "        self.linear=nn.Linear(d_model, vocab_size)\n",
    "    def forward(self , x ):\n",
    "        # (batch , seq_len , d_model) -> (batch , seq_len , vocab_size)\n",
    "        return torch.log_softmax(self.linear(x) , dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder : Encoder , decoder : Dencoder ,srcEmbedding  : Inputembedding ,targettembedding   : Inputembedding , src_position : PositionalEncodimg , target_position : PositionalEncodimg, projection = ProjectionLayer ):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.srcEmbedding = srcEmbedding\n",
    "        self.targettembedding = targettembedding\n",
    "        self.src_position = src_position\n",
    "        self.target_position = target_position\n",
    "        self.projection=projection\n",
    "    def encoder(self , src , src_mask ):\n",
    "        src = self.srcEmbedding(src)\n",
    "        src = self.src_position(src)\n",
    "        return self.encoder(src , src_mask)\n",
    "    def decode(self , tgt , tgt_mask):\n",
    "        tgt = self.targettembedding(tgt)\n",
    "        tgt = self.target_position(tgt)\n",
    "        return self.encoder(tgt , tgt_mask)\n",
    "    def project(self , x):\n",
    "        return self.project(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer( src_vocab_size : int , tgt_vocab_size : int , src_seq_len : int , tgt_seq_len : int , d_model :int = 512 , n : int = 6  ,  h : int  = 8  , dropout : int =0.1 , d_ff : int = 2048 ) -> Transformer:\n",
    "    \n",
    "    src_embed = Inputembedding(d_model , src_vocab_size)\n",
    "    tgt_emdeb = Inputembedding(d_model , tgt_vocab_size)\n",
    "\n",
    "    src_position = PositionalEncodimg(d_model,src_seq_len , dropout)\n",
    "    tgt_position = PositionalEncodimg(d_model,tgt_seq_len , dropout)\n",
    "\n",
    "    # creat n encoder \n",
    "\n",
    "    encoder_bloks=[]\n",
    "    for _ in  range(n):\n",
    "        encoder_self_attention = MultiheadAttenstion(d_model,h,dropout)\n",
    "        feed_Forword = Feed_Forword(d_model,d_ff,dropout)\n",
    "        encoder_blok = EncoderBlock(encoder_self_attention,feed_Forword,dropout)\n",
    "        encoder_bloks.append(encoder_blok)\n",
    "    \n",
    "    # decoder \n",
    "    dencoder_bloks=[]\n",
    "    for _ in  range(n):\n",
    "        encoder_self_attention = MultiheadAttenstion(d_model,h,dropout)\n",
    "        feed_Forword = Feed_Forword(d_model,d_ff,dropout)\n",
    "        dencoder_blok = DencoderBlock(encoder_self_attention,feed_Forword,dropout)\n",
    "        dencoder_bloks.append(dencoder_blok)\n",
    "    \n",
    "    encoder = Encoder(nn.ModuleList(encoder_bloks) )\n",
    "    decoder = Dencoder(nn.ModuleList(dencoder_bloks))\n",
    "    # projection layer \n",
    "    project = ProjectionLayer(d_model , tgt_vocab_size) \n",
    "    # tamsformer \n",
    "    transformer = Transformer(encoder,decoder,src_embed,tgt_emdeb,src_position,tgt_position,project)\n",
    "    # initia the parammeters \n",
    "    for p in transformer.parameters():\n",
    "        if p.dim()>1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
